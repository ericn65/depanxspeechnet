{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE OVER 5 CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_Loader import dataLoaderPickle, justDatasetLoaderPickle\n",
    "from networks_v2 import AttentionModel\n",
    "from optimizer import train2, test2, optimizerNet\n",
    "from optimizer import IterMeter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, SubsetRandomSampler, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = justDatasetLoaderPickle('BALANCED_DATASET_ENG_SCRIPTED_FULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model, device, train_data, criterion, optimizer, scheduler, epochs, iter_meter, experiment):\n",
    "    loss_list = []\n",
    "    labels_list = []\n",
    "    loss_tot_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_data:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            z = model(x.float())\n",
    "            labels_list.extend(y.detach().numpy())\n",
    "            z = F.log_softmax(z, dim=1)\n",
    "\n",
    "            y = y.float()\n",
    "            #z = z.squeeze(0)\n",
    "\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_meter.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "        print('Train Epoch: {} \\tLoss: {:.4f}\\tRMSE: {:.4f}'.format(\n",
    "            epoch,\n",
    "            np.mean(loss_list),\n",
    "            np.sqrt(np.mean(loss_list))\n",
    "        ))\n",
    "        loss_tot_list.append(np.mean(loss_list))\n",
    "    \n",
    "    #Printing training behaviour \n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].plot(range(epochs), loss_tot_list)\n",
    "    axs[0].set_title('Training Loss')\n",
    "    axs[0].set(xlabel= 'Epoch',ylabel='Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(model, device, test_data, criterion, optimizer, scheduler, epochs, iter_meter, experiment):   \n",
    "    loss_list = []\n",
    "    labels_list = []\n",
    "    loss_tot_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in test_data:\n",
    "            optimizer.zero_grad()\n",
    "           \n",
    "            z = model(x.float())\n",
    "            labels_list.extend(y.detach().numpy())\n",
    "            z = F.log_softmax(z, dim=0)\n",
    "\n",
    "            y = y.float()\n",
    "            z = z.squeeze(0)\n",
    "            loss = criterion(z, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_meter.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "        print('Test Epoch: {} \\tLoss: {:.4f}\\tRMSE: {:.4f}'.format(\n",
    "            epoch,\n",
    "            np.mean(loss_list),\n",
    "            np.sqrt(np.mean(loss_list))\n",
    "        ))\n",
    "        loss_tot_list.append(np.mean(loss_list))\n",
    "\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].plot(range(epochs), loss_tot_list)\n",
    "    axs[0].set_title('Test Loss')\n",
    "    axs[0].set(xlabel= 'Epoch',ylabel='Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    }
   ],
   "source": [
    "#Constants\n",
    "learning_Rate = 0.0005\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "experiment = Experiment(api_key='dummy_key', disabled=True)\n",
    "\n",
    "hparams = {\n",
    "    \"n_cnn_layers\": 6,\n",
    "    \"n_rnn_layers\": 1,\n",
    "    \"rnn_dim\": 256,\n",
    "    \"h_rnn_layers\": 128,\n",
    "    \"n_class\": 1,\n",
    "    \"n_feats\": 64,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"learning_rate\": learning_Rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "\n",
    "experiment.log_parameters(hparams)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(7)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionModel(\n",
      "  (dense): Conv1d(500, 32, kernel_size=(1,), stride=(1,))\n",
      "  (cnn): Sequential(\n",
      "    (0): ResCNN(\n",
      "      (conv1): Sequential(\n",
      "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): ResCNN(\n",
      "      (conv1): Sequential(\n",
      "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): ResCNN(\n",
      "      (conv1): Sequential(\n",
      "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3): ResCNN(\n",
      "      (conv1): Sequential(\n",
      "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (4): ResCNN(\n",
      "      (conv1): Sequential(\n",
      "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (5): ResCNN(\n",
      "      (conv1): Sequential(\n",
      "        (0): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dense2): Linear(in_features=1280, out_features=256, bias=True)\n",
      "  (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "  (atten): MyLSTM(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (lstm1): LSTM(\n",
      "      (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (atten1): Attention()\n",
      "    (lstm2): LSTM(\n",
      "      (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (atten2): Attention()\n",
      "  )\n",
      "  (fc): FullyConnected(\n",
      "    (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "Number Model Parameters 1544097\n"
     ]
    }
   ],
   "source": [
    "#Building and training Model\n",
    "model = AttentionModel(\n",
    "    hparams['n_cnn_layers'],\n",
    "    hparams['rnn_dim'],\n",
    "    hparams['h_rnn_layers'],\n",
    "    hparams['n_rnn_layers'],\n",
    "    hparams['n_class'],\n",
    "    hparams['stride'],\n",
    "    hparams['dropout']\n",
    ").to(device).float()\n",
    "\n",
    "print(model)\n",
    "print('Number Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "#Optimizing Model\n",
    "optimizer, scheduler = optimizerNet(model, hparams)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "iter_meter = IterMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 \tLoss: 5.8828\tRMSE: 2.4254\n",
      "Train Epoch: 1 \tLoss: 5.8828\tRMSE: 2.4255\n",
      "Train Epoch: 2 \tLoss: 5.8828\tRMSE: 2.4254\n",
      "Train Epoch: 3 \tLoss: 5.8826\tRMSE: 2.4254\n",
      "Train Epoch: 4 \tLoss: 5.8826\tRMSE: 2.4254\n",
      "Train Epoch: 5 \tLoss: 5.8826\tRMSE: 2.4254\n",
      "Train Epoch: 6 \tLoss: 5.8827\tRMSE: 2.4254\n",
      "Train Epoch: 7 \tLoss: 5.8827\tRMSE: 2.4254\n",
      "Train Epoch: 8 \tLoss: 5.8827\tRMSE: 2.4254\n",
      "Train Epoch: 9 \tLoss: 5.8827\tRMSE: 2.4254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 0 \tLoss: 31.8332\tRMSE: 5.6421\n",
      "Test Epoch: 1 \tLoss: 31.8485\tRMSE: 5.6434\n",
      "Test Epoch: 2 \tLoss: 31.8679\tRMSE: 5.6452\n",
      "Test Epoch: 3 \tLoss: 31.8667\tRMSE: 5.6451\n",
      "Test Epoch: 4 \tLoss: 31.8763\tRMSE: 5.6459\n",
      "Test Epoch: 5 \tLoss: 31.8787\tRMSE: 5.6461\n",
      "Test Epoch: 6 \tLoss: 31.8855\tRMSE: 5.6467\n",
      "Test Epoch: 7 \tLoss: 31.8803\tRMSE: 5.6463\n",
      "Test Epoch: 8 \tLoss: 31.8790\tRMSE: 5.6461\n",
      "Test Epoch: 9 \tLoss: 31.8786\tRMSE: 5.6461\n",
      "Fold 2\n",
      "Train Epoch: 0 \tLoss: 5.9006\tRMSE: 2.4291\n",
      "Train Epoch: 1 \tLoss: 5.9005\tRMSE: 2.4291\n",
      "Train Epoch: 2 \tLoss: 5.9008\tRMSE: 2.4291\n",
      "Train Epoch: 3 \tLoss: 5.9009\tRMSE: 2.4292\n",
      "Train Epoch: 4 \tLoss: 5.9009\tRMSE: 2.4292\n",
      "Train Epoch: 5 \tLoss: 5.9008\tRMSE: 2.4292\n",
      "Train Epoch: 6 \tLoss: 5.9008\tRMSE: 2.4292\n",
      "Train Epoch: 7 \tLoss: 5.9008\tRMSE: 2.4291\n",
      "Train Epoch: 8 \tLoss: 5.9008\tRMSE: 2.4292\n",
      "Train Epoch: 9 \tLoss: 5.9007\tRMSE: 2.4291\n",
      "Test Epoch: 0 \tLoss: 31.4808\tRMSE: 5.6108\n",
      "Test Epoch: 1 \tLoss: 31.4876\tRMSE: 5.6114\n",
      "Test Epoch: 2 \tLoss: 31.4792\tRMSE: 5.6106\n",
      "Test Epoch: 3 \tLoss: 31.4743\tRMSE: 5.6102\n",
      "Test Epoch: 4 \tLoss: 31.4716\tRMSE: 5.6100\n",
      "Test Epoch: 5 \tLoss: 31.4688\tRMSE: 5.6097\n",
      "Test Epoch: 6 \tLoss: 31.4683\tRMSE: 5.6097\n",
      "Test Epoch: 7 \tLoss: 31.4704\tRMSE: 5.6098\n",
      "Test Epoch: 8 \tLoss: 31.4666\tRMSE: 5.6095\n",
      "Test Epoch: 9 \tLoss: 31.4657\tRMSE: 5.6094\n",
      "Fold 3\n",
      "Train Epoch: 0 \tLoss: 5.9150\tRMSE: 2.4321\n",
      "Train Epoch: 1 \tLoss: 5.9150\tRMSE: 2.4321\n",
      "Train Epoch: 2 \tLoss: 5.9150\tRMSE: 2.4321\n",
      "Train Epoch: 3 \tLoss: 5.9150\tRMSE: 2.4321\n",
      "Train Epoch: 4 \tLoss: 5.9152\tRMSE: 2.4321\n",
      "Train Epoch: 5 \tLoss: 5.9151\tRMSE: 2.4321\n",
      "Train Epoch: 6 \tLoss: 5.9152\tRMSE: 2.4321\n",
      "Train Epoch: 7 \tLoss: 5.9152\tRMSE: 2.4321\n",
      "Train Epoch: 8 \tLoss: 5.9151\tRMSE: 2.4321\n",
      "Train Epoch: 9 \tLoss: 5.9151\tRMSE: 2.4321\n",
      "Test Epoch: 0 \tLoss: 31.1331\tRMSE: 5.5797\n",
      "Test Epoch: 1 \tLoss: 31.0993\tRMSE: 5.5767\n",
      "Test Epoch: 2 \tLoss: 31.0838\tRMSE: 5.5753\n",
      "Test Epoch: 3 \tLoss: 31.0835\tRMSE: 5.5753\n",
      "Test Epoch: 4 \tLoss: 31.0797\tRMSE: 5.5749\n",
      "Test Epoch: 5 \tLoss: 31.0824\tRMSE: 5.5752\n",
      "Test Epoch: 6 \tLoss: 31.0781\tRMSE: 5.5748\n",
      "Test Epoch: 7 \tLoss: 31.0792\tRMSE: 5.5749\n",
      "Test Epoch: 8 \tLoss: 31.0771\tRMSE: 5.5747\n",
      "Test Epoch: 9 \tLoss: 31.0804\tRMSE: 5.5750\n",
      "Fold 4\n",
      "Train Epoch: 0 \tLoss: 5.8878\tRMSE: 2.4265\n",
      "Train Epoch: 1 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Train Epoch: 2 \tLoss: 5.8878\tRMSE: 2.4265\n",
      "Train Epoch: 3 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Train Epoch: 4 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Train Epoch: 5 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Train Epoch: 6 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Train Epoch: 7 \tLoss: 5.8880\tRMSE: 2.4265\n",
      "Train Epoch: 8 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Train Epoch: 9 \tLoss: 5.8879\tRMSE: 2.4265\n",
      "Test Epoch: 0 \tLoss: 31.8594\tRMSE: 5.6444\n",
      "Test Epoch: 1 \tLoss: 31.8232\tRMSE: 5.6412\n",
      "Test Epoch: 2 \tLoss: 31.8106\tRMSE: 5.6401\n",
      "Test Epoch: 3 \tLoss: 31.8094\tRMSE: 5.6400\n",
      "Test Epoch: 4 \tLoss: 31.8150\tRMSE: 5.6405\n",
      "Test Epoch: 5 \tLoss: 31.8103\tRMSE: 5.6401\n",
      "Test Epoch: 6 \tLoss: 31.8101\tRMSE: 5.6400\n",
      "Test Epoch: 7 \tLoss: 31.8116\tRMSE: 5.6402\n",
      "Test Epoch: 8 \tLoss: 31.8120\tRMSE: 5.6402\n",
      "Test Epoch: 9 \tLoss: 31.8099\tRMSE: 5.6400\n",
      "Fold 5\n",
      "Train Epoch: 0 \tLoss: 5.8782\tRMSE: 2.4245\n",
      "Train Epoch: 1 \tLoss: 5.8782\tRMSE: 2.4245\n",
      "Train Epoch: 2 \tLoss: 5.8783\tRMSE: 2.4245\n",
      "Train Epoch: 3 \tLoss: 5.8784\tRMSE: 2.4245\n",
      "Train Epoch: 4 \tLoss: 5.8784\tRMSE: 2.4246\n",
      "Train Epoch: 5 \tLoss: 5.8785\tRMSE: 2.4246\n",
      "Train Epoch: 6 \tLoss: 5.8784\tRMSE: 2.4245\n",
      "Train Epoch: 7 \tLoss: 5.8784\tRMSE: 2.4246\n",
      "Train Epoch: 8 \tLoss: 5.8785\tRMSE: 2.4246\n",
      "Train Epoch: 9 \tLoss: 5.8785\tRMSE: 2.4246\n",
      "Test Epoch: 0 \tLoss: 32.0980\tRMSE: 5.6655\n",
      "Test Epoch: 1 \tLoss: 32.0714\tRMSE: 5.6632\n",
      "Test Epoch: 2 \tLoss: 32.0666\tRMSE: 5.6627\n",
      "Test Epoch: 3 \tLoss: 32.0721\tRMSE: 5.6632\n",
      "Test Epoch: 4 \tLoss: 32.0763\tRMSE: 5.6636\n",
      "Test Epoch: 5 \tLoss: 32.0704\tRMSE: 5.6631\n",
      "Test Epoch: 6 \tLoss: 32.0730\tRMSE: 5.6633\n",
      "Test Epoch: 7 \tLoss: 32.0701\tRMSE: 5.6630\n",
      "Test Epoch: 8 \tLoss: 32.0741\tRMSE: 5.6634\n",
      "Test Epoch: 9 \tLoss: 32.0722\tRMSE: 5.6632\n",
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 \tLoss: 5.9042\tRMSE: 2.4299\n",
      "Train Epoch: 1 \tLoss: 5.9044\tRMSE: 2.4299\n",
      "Train Epoch: 2 \tLoss: 5.9044\tRMSE: 2.4299\n",
      "Train Epoch: 3 \tLoss: 5.9044\tRMSE: 2.4299\n",
      "Train Epoch: 4 \tLoss: 5.9044\tRMSE: 2.4299\n",
      "Train Epoch: 5 \tLoss: 5.9044\tRMSE: 2.4299\n",
      "Train Epoch: 6 \tLoss: 5.9043\tRMSE: 2.4299\n",
      "Train Epoch: 7 \tLoss: 5.9043\tRMSE: 2.4299\n",
      "Train Epoch: 8 \tLoss: 5.9043\tRMSE: 2.4299\n",
      "Train Epoch: 9 \tLoss: 5.9043\tRMSE: 2.4299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:530: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 0 \tLoss: 31.2543\tRMSE: 5.5906\n",
      "Test Epoch: 1 \tLoss: 31.2578\tRMSE: 5.5909\n",
      "Test Epoch: 2 \tLoss: 31.2698\tRMSE: 5.5919\n",
      "Test Epoch: 3 \tLoss: 31.2789\tRMSE: 5.5928\n",
      "Test Epoch: 4 \tLoss: 31.2839\tRMSE: 5.5932\n",
      "Test Epoch: 5 \tLoss: 31.2849\tRMSE: 5.5933\n",
      "Test Epoch: 6 \tLoss: 31.2871\tRMSE: 5.5935\n",
      "Test Epoch: 7 \tLoss: 31.2878\tRMSE: 5.5935\n",
      "Test Epoch: 8 \tLoss: 31.2974\tRMSE: 5.5944\n",
      "Test Epoch: 9 \tLoss: 31.2935\tRMSE: 5.5941\n",
      "Fold 7\n",
      "Train Epoch: 0 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 1 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 2 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 3 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 4 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 5 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 6 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 7 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Train Epoch: 8 \tLoss: 5.8823\tRMSE: 2.4254\n",
      "Train Epoch: 9 \tLoss: 5.8823\tRMSE: 2.4253\n",
      "Test Epoch: 0 \tLoss: 31.8161\tRMSE: 5.6406\n",
      "Test Epoch: 1 \tLoss: 31.7978\tRMSE: 5.6390\n",
      "Test Epoch: 2 \tLoss: 31.7832\tRMSE: 5.6377\n",
      "Test Epoch: 3 \tLoss: 31.7895\tRMSE: 5.6382\n",
      "Test Epoch: 4 \tLoss: 31.7890\tRMSE: 5.6382\n",
      "Test Epoch: 5 \tLoss: 31.7812\tRMSE: 5.6375\n",
      "Test Epoch: 6 \tLoss: 31.7753\tRMSE: 5.6370\n",
      "Test Epoch: 7 \tLoss: 31.7708\tRMSE: 5.6366\n",
      "Test Epoch: 8 \tLoss: 31.7674\tRMSE: 5.6363\n",
      "Test Epoch: 9 \tLoss: 31.7664\tRMSE: 5.6362\n",
      "Fold 8\n",
      "Train Epoch: 0 \tLoss: 5.8633\tRMSE: 2.4214\n",
      "Train Epoch: 1 \tLoss: 5.8632\tRMSE: 2.4214\n",
      "Train Epoch: 2 \tLoss: 5.8632\tRMSE: 2.4214\n",
      "Train Epoch: 3 \tLoss: 5.8633\tRMSE: 2.4214\n",
      "Train Epoch: 4 \tLoss: 5.8633\tRMSE: 2.4214\n",
      "Train Epoch: 5 \tLoss: 5.8633\tRMSE: 2.4214\n",
      "Train Epoch: 6 \tLoss: 5.8633\tRMSE: 2.4214\n",
      "Train Epoch: 7 \tLoss: 5.8633\tRMSE: 2.4214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, sampler\u001b[39m=\u001b[39mtrain_sampler)\n\u001b[0;32m      9\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, sampler\u001b[39m=\u001b[39mtest_sampler)\n\u001b[1;32m---> 11\u001b[0m train2(model, device, train_loader, criterion, optimizer, scheduler, epochs, iter_meter, experiment)\n\u001b[0;32m     12\u001b[0m test2(model, device, test_loader, criterion, optimizer, scheduler, epochs, iter_meter, experiment)\n",
      "Cell \u001b[1;32mIn [3], line 18\u001b[0m, in \u001b[0;36mtrain2\u001b[1;34m(model, device, train_data, criterion, optimizer, scheduler, epochs, iter_meter, experiment)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m#z = z.squeeze(0)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[39m=\u001b[39m criterion(z, y)\n\u001b[1;32m---> 18\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m iter_meter\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\ericq\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {'Train loss':[], 'Test Loss':[], 'Train Accuracy':[], 'Test Accuracy':[]}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "    print('Fold {}'.format(fold+1))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "    train2(model, device, train_loader, criterion, optimizer, scheduler, epochs, iter_meter, experiment)\n",
    "    test2(model, device, test_loader, criterion, optimizer, scheduler, epochs, iter_meter, experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bdda07c8f3c0d50e657417337633c7d770439b214bea89575d6ab5962ed7357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
